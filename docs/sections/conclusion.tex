\chapter{Conclusion}
\label{ch:Conclusion}

This work had two goals. The first goal was to introduce a teacher to the reinforcement learning framework. The second was to develop new algorithms to train RL agents that safely interact with their environment.

The teacher was introduced with the preference reward algorithm, a reward shaping technique that combines the reward signal from the environment with a new reward function for the teacher. The teacher was defined as a function that maps a state to an action and a confidence about the quality of this action. The teachers action is then compared to the students action using the mean square error, to form the teacher reward. 

For the second goal, we utilized a safety critic. Similar to the reward critic from SAC, the safety critic is trained to estimate the expected cost for the remainder of the episode. Based on the safety critic we proposed to new approaches and compared them to an established reward based approach that deducts the cost from the reward when training the agent. The two safety critic based approaches are safety training and safety evaluation. The safety training algorithm changes the training objective of SAC, to optimize the reward and cost together. In contrast, the safety evaluation algorithm does not change the training. The goal with this algorithm was not to reduce the cost during training, but to learn how to avoid cost during evaluation. To do so, we utilized the stochastic nature of SAC policies to draw multiple actions from the policy and execute the one with lowest cost.

Afterwards we evaluated the three algorithms on two environments form the OpenAI Gym robotics environments as well as a custom environment. The two environment from OpenAI Gym were FetchReach and FetchPush. Based on the FetchPush environment we developed the FetchPushBarrier environment, where the object and the target are always separated by a barrier, the agent has to move the object around.

On the FetchReach environment, we saw that adding a teacher lead to improvements in the success rate when training SAC using pixels and also when adding an autoencoder as an auxiliary loss. When using DrQ to improve the performance of SAC pixel, the agent was able to solve the FetchReach environment without the help of teacher. However, with a teacher we were able to significantly improve the sample efficiency.

For the experiments on the FetchPush environment, we only trained agents with DrQ, since only those were able to reliably solve the FetchReach environment. We saw that an agent trained without the teacher was not able to solve the environment. We then continued to train multiple agents with different values for $\alpha$, the parameter that marks the importance of the teacher. We saw that only trusting the teacher and not using the environment reward at all did not lead to the best results. With an $\alpha$ value of $0.5$ we achieved the best results. We achieved similar results with an adaptive $\alpha$. A function that reduces the $\alpha$ value when the environment reward increases, indicating that the agent learned something.

On the FetchPushBarrier environment, we first trained agents using the full state information, to evaluate the safety training and safety evaluation algorithms. We saw that agents trained with the safety training algorithm were able to outperform the agents trained with the reward based approach we used to compare our new algorithms to. With the safety, training we saw similar, but slightly better success rates, as well as significantly lower episode costs. The agents trained using the safety evaluation algorithm, however, lacked behind, both in the success rate and the episode costs. While the safety evaluation was able to reduce the episode costs in contrast to not using it, the costs are still much higher than using the reward based approach or the safety training algorithm.

Lastly, we performed experiments on the FetchPushBarrier environment using high dimensional image observations. We were not able to reliably solve the task with any configuration we tried. The configuration varied in the safety algorithm we used, as well as the $\alpha$ value. However, we saw that the agents did learn to move the object and even move it around the barrier. We suspect that this is due to the lower quality of the teacher when compared to the teachers of the FetchReach and FetchPush environment. We leave it up to future work to show if an improved teacher can lead to better performance on the FetchPushBarrier environment using high dimensional image observations.

Overall, we saw significant improvements in the sample efficiency or success rate on the FetchReach and FetchPush environments when using the teacher. We were not able to solve the FetchPushBarrier environment when using pixel observations, even with a teacher. However, it unclear if this due to the difficulty of the environment or the fact that the teacher for the FetchPushBarrier environment is significantly worse than the teachers for the other environments. This leaves room for future work to explore the performance on the FetchPushBarrier environment with an improved teacher. A possible solution could be as simple as improving the performance of SAC state on the FetchPushBarrier environment. This could be done for example by using a hindsight experience replay buffer \cite{andrychowiczHindsightExperienceReplay2018}.

In the following we will propose further improvements and experiments that can be explored in future work. Similar to the improved ensemble teacher mentioned above, other teachers could improve the performance. This includes classic control methods like trajectory planners as well as human based teachers. The benefit of these teachers is that they are easier to use in real world scenarios, where it might not be viable to first train multiple agents using the state space. However, these teachers require a new approach to measure the confidence in the actions.

Other improvement we see are in the choice of the parameter $\alpha$. In this work we either set a fixed value or used a linear function based on the environment reward. The first step could be to try different functions that for example decrease the value faster as the environment reward increases. Also another metric could be used to measure how much the agent has learned already and lower the $\alpha$ value accordingly. Another possibility is to directly optimize the $\alpha$ value, similar to the temperature in SAC that defines the importance of the entropy in contrast to the reward. As described in section \ref{sec:results:fetch-push}, an adaptive $\alpha$ can lead to lower preference rewards even when the agent performs better with regards to the environment reward. With off-policy RL algorithms this is not ideal for training. We see a possible solution to this problem by storing both the environment and the teacher reward in the replay buffer. The preference reward is then calculated only when it is needed with the current $\alpha$ value. However, we leave the evaluation of this possible solution for future work.

The most drastic improvement we propose is the combination of the preference reward algorithm with model base RL. In model based RL, a dynamics model is learned to predict the next state of the environment before an action is executed. The learned dynamics model could be used to predict the effect of the teachers action and therefore measure the quality. The importance of the teacher can then be set according to the predicted quality.

Finally, it would be interesting to see the impact of the preference reward algorithm on experiments with real robots. As stated above, it is probably required to utilize a different teacher to the ensemble teacher, used in this work, to avoid the need to train multiple RL agents. The ensemble teacher would also require a lot of additional sensors to measure the state space we had available in the simulation.
