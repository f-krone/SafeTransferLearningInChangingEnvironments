\chapter{Introduction}
\label{ch:Introduction}

Learning through interaction, this is the paradigm that is usually used to describe reinforcement learning (RL). An agent interacts with an environment and receives a reward signal for its actions. The agent then learns to maximize the reward signal. This basic idea behind reinforcement learning is derived from the way humans learn. \cite{suttonReinforcementLearningIntroduction2018}
At least, this is what is often used to motivate reinforcement learning. However, this explanation forgets an essential part of the learning process. A child does not just start to speak because it taught itself to do so but imitates its parents. Thus the parents act as a teacher for the child. For more complex tasks, we actually employ teachers. Be it in school, or as driving instructors, for example. We do not just learn to drive a car but someone tells us how to do so. Although we might be able to learn how to drive purely from experience, it would be a massive security risk.

With this in mind, reinforcement learning clearly lacks the notion of a teacher. We expect an RL agent to learn complex tasks purely through interaction. In this work, we explore the possibility of extending the reinforcement learning framework with a teacher. The teacher is hidden behind a simple interface. This allows us to develop our approach agnostic to the actual teacher used in practice.

Obviously, a teacher requires knowledge about the task and how to solve it. A possible, yet very simple, the teacher can be designed around a human as a source of knowledge. However, RL algorithms tend to require a lot of samples to learn complex tasks, making this approach very tedious. Another possibility is to utilize another control algorithm, like trajectory optimization, as a teacher. Even other RL agents can be used as teachers.

One might question the need to train an RL agent using a teacher, if we already have a teacher at hand that can solve the same task. The motivation to train the RL agent is manifold. The teacher may only be able to solve a sub-task of the task the RL agent is trained on. The teacher can then still be used to guide the exploration of the RL agent. Another option is to use a different observation for the teacher and the student. If the student uses only images as observation, only a camera is needed as a sensor when using the agent in practice. To help the training, the teacher may use the information from additional sensors. This makes it cheaper to use the student in practice than the teacher. But even if the student and the teacher use the same observations to solve the same task, we benefit from having an RL agent. With modern RL algorithms, we get a stochastic policy that can adapt to changes in the environment. Also, the continued interaction with the environment when using the student in practice can be used to train the agent without a teacher further.

Typically, reinforcement learning aims to solve a task by maximizing a reward function. Additionally, a second goal arises when applying reinforcement learning to real-world problems, especially robotic tasks like those considered in this work. The agent needs to interact with its environment safely. A very obvious safety risk lies in the interaction with humans for a robotic control task. The robot should never harm a human. The second contribution of this work is the development of two novel algorithms to train an agent so that it can be safely used in real-world scenarios.

The remainder of this work is structured as follows. In chapter \ref{ch:preliminaries} we establish the background needed for this work. This includes reinforcement learning in general and the specific algorithms used in his work. Following the preliminaries, we give an overview of related works in chapter \ref{ch:relatedWork}. Chapter \ref{ch:algorithm} first introduces the aforementioned teacher in the preference reward algorithm. Afterward, we propose two novel algorithms for safe RL, safety training, and safety evaluation. All algorithms are evaluated in chapter \ref{ch:experiments} with many experiments on three robotic control environments. We conclude in chapter \ref{ch:Conclusion} with a summary of the approaches and the results. Additionally, we give an outlook on possible future steps.