%% LaTeX2e class for student theses
%% sections/abstract_en.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.5, 2020-06-26

\Abstract
Deep reinforcement learning has shown impressive results when using complex features as observations. In recent years, the performance of agents using high dimensional image observations was improved significantly. However, it still lacks behind for complex tasks. In this work, we explore a novel approach to guide the exploration of an agent. We show that this can be utilized to significantly improve the performance of agents trained on high dimensional image observations. We call our approach the Preference Reward algorithm. Our approach uses reward shaping and can therefore be used with any reinforcement learning algorithm. The second major problem we focused on in this work is safe reinforcement learning. In robotic environments, the agent must be constrained to not reach certain states that for example could hurt humans or the robot itself. We propose two new algorithms to train agents with such constraints in mind. The algorithms are called Safety Training and Safety Evaluation. Our algorithms are designed around actor critic methods. Specifically, we evaluated them with a Soft Actor-Critic (SAC).