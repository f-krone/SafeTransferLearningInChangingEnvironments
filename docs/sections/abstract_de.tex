%% LaTeX2e class for student theses
%% sections/abstract_de.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.5, 2020-06-26

\Abstract
Mit Deep Reinforcement Learning sind beeindruckende Ergebnisse möglich, wenn komplexe Features als Observationen verwendet werden. In den letzten Jahren wurde die Leistung von Agenten, die hochdimensionale Bildobservationen verwenden deutlich verbessert. Bei komplexen Aufgaben liegt sie jedoch noch immer deutlich unter derer von Agenten mit komplexen Fetarures als Observationen. In dieser Arbeit untersuchen wir einen neuartigen Ansatz, um die Erkundung eines Agenten zu steuern. Wir zeigen, dass dieser Ansatz die Leistung von Agenten, die auf hochdimensionalen Bildobservationen trainiert werden, deutlich verbessern kann. Wir nennen unseren Ansatz den Preference Reward Algorithmus. Unser Ansatz verwendet Reward-Shaping und kann daher mit jedem Reinforcement Learning Algorithmus verwendet werden. Das zweite große Problem, auf das wir uns in dieser Arbeit konzentrieren haben, ist safe Reinforcement Learning. In Roboterumgebungen darf der Agent bestimmte bestimmte Zustände nicht erreichen, die zum Beispiel Menschen oder den Roboter selbst verletzen könnten. Wir schlagen zwei neue Algorithmen vor, um Agenten unter Berücksichtigung solcher Einschränkungen zu trainieren. Wir nennen die Algorithmen Safety Training und Safety Evaluation. Unsere Algorithmen sind auf der Grundlage von Actor Critic Methoden entwickelt. Insbesondere haben wir sie mit einer Soft Actor-Critic (SAC) evaluiert.

