\chapter{Related work}
\label{ch:relatedWork}

This work combines  three active fields of research in reinforcement learning (RL), Transfer Learning, Safe RL and the usage of pixel observations. The following sections will provide an overview of different approaches in these fields.

\section{Transfer Learning}
\label{sec:transferLearning}

Transfer learning is very well known in supervised learning. It usually refers to using a pre trained model, or a part of it, and fine tune it on a new problem, reducing the training time on the new problem. This approach has been used for RL as well, for example by \citeauthor{delacruzInitialProgressTransfer2016} \cite{delacruzInitialProgressTransfer2016}. They were able to improve the performance in some Atari games, by transferring some layers from agents, trained on other Atari games. \citeauthor{gamrianTransferLearningRelated2019a} \cite{gamrianTransferLearningRelated2019a} found that an agent trained on the first level of the Nintendo game Road Fighter was not able to complete subsequent levels. However, when mapping the images back to the familiar images from the first level, using GANs, the agent was able to complete the level. Thus, they were able to transfer knowledge from one task to another.

\section{Safe Reinforcement Learning}
\label{sec:safeRL}

\citeauthor{guReviewSafeReinforcement2022} \cite{guReviewSafeReinforcement2022} defined five problems, called \enquote{2H3W}, in safe RL. The problems are safety policy, complexity, applications, benchmarks, and challenges. Our work addresses the safety policy problem, formulated as \enquote{How can we perform policy optimization to search for a safe policy?}\cite{guReviewSafeReinforcement2022} Therefore, this section focuses on solutions for this problem.

Often, the problem is formulated as a Constrained Markov Decision Process (CMDP), as described in Section \ref{sec:preliminaries:rl:cmdp}. From this, a Lagrangian function can be formulated, combining the reward function with the constraints. A number of Algorithms have been proposed to solve a CMDP with this approach \cite{borkarActorcriticAlgorithmConstrained2005, yuConvergentPolicyOptimization2019, tesslerRewardConstrainedPolicy2018, paternainConstrainedReinforcementLearning2019, paternainSafePoliciesReinforcement2022}. \citeauthor{bharadhwajConservativeSafetyCritics2021} \cite{bharadhwajConservativeSafetyCritics2021} combined this approach with a safety critic to estimate the expected cost, similar to our approach. However, they only use the critic during the sampling of actions. Others adapted trust region methods to also consider safety constraints \cite{achiamConstrainedPolicyOptimization2017}. 

A number of algorithms have been developed that attempt to train for reward and safety in different steps. \citeauthor{yangProjectionBasedConstrainedPolicy2020} \cite{yangProjectionBasedConstrainedPolicy2020} build upon TRPO to first learn a policy that optimizes the reward and then projecting it back to the closest policy that fulfills the constraints. Similarly, \citeauthor{wagenerSafeReinforcementLearning2021} \cite{wagenerSafeReinforcementLearning2021} developed an algorithm that is agnostic of the underlying RL algorithm. They learn a policy agnostic of safety, but before executing the actions in the environment, they are run through an intervention policy and potentially changed to ensure safety. A similar approach was followed by \citeauthor{phamOptLayerPracticalConstrained2018} \cite{phamOptLayerPracticalConstrained2018}. They developed an OptLayer, that computes a safe action, closest to the original prediction.  \citeauthor{wachiSafeReinforcementLearning2020} \cite{wachiSafeReinforcementLearning2020} proposed an algorithm that first expands a safety region, and then explores and exploits the region to optimize the reward.

\section{Pixel Observations}
\label{sec:pixelObservations}

Training RL agents on high dimensional image observations has proven to be less sample efficient and the agents are usually performing worse than their counterparts trained on the complete state information \cite{yaratsImprovingSampleEfficiency2020, kostrikovImageAugmentationAll2021, srinivasCURLContrastiveUnsupervised2020}. It is an active field of research, how to close the gap to state based agents.

\citeauthor{kostrikovImageAugmentationAll2021} \cite{kostrikovImageAugmentationAll2021} investigated the effects of image augmentations on the performance and sample efficiency. Additionally, they proposed novel regularization techniques for the target Q and Q function, by averaging over multiple samples, obtained through different augmentations on a single observation.

\citeauthor{yaratsImprovingSampleEfficiency2020} \cite{yaratsImprovingSampleEfficiency2020} used an autoencoder to aid the agents ability to learn a latent space from the pixel observations. Similarly, \citeauthor{srinivasCURLContrastiveUnsupervised2020} \cite{srinivasCURLContrastiveUnsupervised2020} applied a contrastive loss as auxiliary loss to improve the encoder. To do so, they use two encoders, each getting different data-augmented versions of the observation. Based on this, \citeauthor{zhanFrameworkEfficientRobotic2020} \cite{zhanFrameworkEfficientRobotic2020} proposed the FERM architecture, where a replay buffer is initialized with human demonstrations, which are then used to pre-train the encoder, using a contrastive loss. Afterwards, the pre-filled replay buffer and pre-trained encoder are used to train a SAC agent. Again, using contrastive loss, \citeauthor{stookeDecouplingRepresentationLearning} \cite{stookeDecouplingRepresentationLearning} aided the training of the encoder by comparing it to an encoded example from $k$ time steps later.

Lastly, when attempting to train under realistic assumptions, images are not the only possible observation. When using a robot arm, the position of the end-effector can be measured, as well as if its is open. A combination of images and a subspace of the complete state, regarding the robot, has been used by \citeauthor{kalashnikovQTOptScalableDeep2018} \cite{kalashnikovQTOptScalableDeep2018} to train robotic manipulation tasks.