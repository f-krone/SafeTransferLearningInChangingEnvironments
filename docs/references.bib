@misc{achiamConstrainedPolicyOptimization2017,
  title = {Constrained {{Policy Optimization}}},
  author = {Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  date = {2017-05-30},
  number = {arXiv:1705.10528},
  eprint = {1705.10528},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1705.10528},
  urldate = {2022-07-18},
  abstract = {For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016; Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016) have enabled new capabilities in highdimensional control, but do not consider the constrained setting.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\CBJ9626T\\Achiam et al. - 2017 - Constrained Policy Optimization.pdf}
}

@misc{achiamConstrainedPolicyOptimization2017a,
  title = {Constrained {{Policy Optimization}}},
  author = {Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  date = {2017-05-30},
  number = {arXiv:1705.10528},
  eprint = {1705.10528},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1705.10528},
  urldate = {2022-08-08},
  abstract = {For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016; Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016) have enabled new capabilities in highdimensional control, but do not consider the constrained setting.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\MQE4QIEH\\Achiam et al. - 2017 - Constrained Policy Optimization.pdf}
}

@misc{andrychowiczHindsightExperienceReplay2018,
  title = {Hindsight {{Experience Replay}}},
  author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  date = {2018-02-23},
  number = {arXiv:1707.01495},
  eprint = {1707.01495},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1707.01495},
  url = {http://arxiv.org/abs/1707.01495},
  urldate = {2022-11-21},
  abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\AK5Z5SZI\\Andrychowicz et al. - 2018 - Hindsight Experience Replay.pdf;C\:\\Users\\Florian Krone\\Zotero\\storage\\PD93YMKV\\1707.html}
}

@misc{bankAutoencoders2021,
  title = {Autoencoders},
  author = {Bank, Dor and Koenigstein, Noam and Giryes, Raja},
  date = {2021-04-03},
  number = {arXiv:2003.05991},
  eprint = {2003.05991},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2003.05991},
  urldate = {2022-06-07},
  abstract = {An autoencoder is a specific type of a neural network, which is mainly designed to encode the input into a compressed and meaningful representation, and then decode it back such that the reconstructed input is similar as possible to the original one. This chapter surveys the different types of autoencoders that are mainly used today. It also describes various applications and use-cases of autoencoders.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\IL5KMLMT\\Bank et al. - 2021 - Autoencoders.pdf}
}

@misc{bharadhwajConservativeSafetyCritics2021,
  title = {Conservative {{Safety Critics}} for {{Exploration}}},
  author = {Bharadhwaj, Homanga and Kumar, Aviral and Rhinehart, Nicholas and Levine, Sergey and Shkurti, Florian and Garg, Animesh},
  date = {2021-04-26},
  number = {arXiv:2010.14497},
  eprint = {2010.14497},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2010.14497},
  urldate = {2022-07-31},
  abstract = {Safe exploration presents a major challenge in reinforcement learning (RL): when active data collection requires deploying partially trained policies, we must ensure that these policies avoid catastrophically unsafe regions, while still enabling trial and error learning. In this paper, we target the problem of safe exploration in RL by learning a conservative safety estimate of environment states through a critic, and provably upper bound the likelihood of catastrophic failures at every training iteration. We theoretically characterize the tradeoff between safety and policy improvement, show that the safety constraints are likely to be satisfied with high probability during training, derive provable convergence guarantees for our approach, which is no worse asymptotically than standard RL, and demonstrate the efficacy of the proposed approach on a suite of challenging navigation, manipulation, and locomotion tasks. Empirically, we show that the proposed approach can achieve competitive task performance while incurring significantly lower catastrophic failure rates during training than prior methods. Videos are at this url https://sites.google.com/view/conservative-safety-critics/home},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\XZJSA8KE\\Bharadhwaj et al. - 2021 - Conservative Safety Critics for Exploration.pdf}
}

@article{borkarActorcriticAlgorithmConstrained2005,
  title = {An Actor-Critic Algorithm for Constrained {{Markov}} Decision Processes},
  author = {Borkar, V. S.},
  date = {2005-03-01},
  journaltitle = {Systems \& Control Letters},
  shortjournal = {Systems \& Control Letters},
  volume = {54},
  number = {3},
  pages = {207--213},
  issn = {0167-6911},
  doi = {10.1016/j.sysconle.2004.08.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0167691104001276},
  urldate = {2022-08-08},
  abstract = {An actor-critic type reinforcement learning algorithm is proposed and analyzed for constrained controlled Markov decision processes. The analysis uses multiscale stochastic approximation theory and the `envelope theorem' of mathematical economics.},
  langid = {english},
  keywords = {Actor-critic algorithms,Constrained Markov decision processes,Envelope theorem,Reinforcement learning,Stochastic approximation},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\TP8UUFNI\\Borkar - 2005 - An actor-critic algorithm for constrained Markov d.pdf;C\:\\Users\\Florian Krone\\Zotero\\storage\\PZBQQN57\\S0167691104001276.html}
}

@misc{chowRiskConstrainedReinforcementLearning2017,
  title = {Risk-{{Constrained Reinforcement Learning}} with {{Percentile Risk Criteria}}},
  author = {Chow, Yinlam and Ghavamzadeh, Mohammad and Janson, Lucas and Pavone, Marco},
  date = {2017-04-06},
  number = {arXiv:1512.01629},
  eprint = {1512.01629},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1512.01629},
  urldate = {2022-08-01},
  abstract = {In many sequential decision-making problems one is interested in minimizing an expected cumulative cost while taking into account risk, i.e., increased awareness of events of small probability and high consequences. Accordingly, the objective of this paper is to present efficient reinforcement learning algorithms for risk-constrained Markov decision processes (MDPs), where risk is represented via a chance constraint or a constraint on the conditional value-at-risk (CVaR) of the cumulative cost. We collectively refer to such problems as percentile risk-constrained MDPs. Specifically, we first derive a formula for computing the gradient of the Lagrangian function for percentile riskconstrained MDPs. Then, we devise policy gradient and actor-critic algorithms that (1) estimate such gradient, (2) update the policy in the descent direction, and (3) update the Lagrange multiplier in the ascent direction. For these algorithms we prove convergence to locally optimal policies. Finally, we demonstrate the effectiveness of our algorithms in an optimal stopping problem and an online marketing application.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\P6XQYIXF\\Chow et al. - 2017 - Risk-Constrained Reinforcement Learning with Perce.pdf}
}

@unpublished{christianoDeepReinforcementLearning2017,
  title = {Deep Reinforcement Learning from Human Preferences},
  author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  date = {2017-07-13},
  eprint = {1706.03741},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1706.03741},
  urldate = {2022-01-07},
  abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1\% of our agent’s interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\G9VH3DKS\\Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf}
}

@inproceedings{delacruzInitialProgressTransfer2016,
  title = {Initial {{Progress}} in {{Transfer}} for {{Deep Reinforcement Learning Algorithms}}},
  author = {de la Cruz, Gabriel and Du, Yunshu and Irwin, James and Taylor, Matthew},
  options = {useprefix=true},
  date = {2016-07-01},
  abstract = {As one of the first successful models that combines reinforcement learning technique with deep neural networks, the Deep Q-network (DQN) algorithm has gained attention as it bridges the gap between high-dimensional sensor inputs and autonomous agent learning. However, one main drawback of DQN is the long training time required to train a single task. This work aims to leverage transfer learning (TL) techniques to speed up learning in DQN. We applied this technique in two domains, Atari games and cart-pole, and show that TL can improve DQN's performance on both tasks without altering the network structure.},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\Y8DT8C2K\\de la Cruz et al. - 2016 - Initial Progress in Transfer for Deep Reinforcemen.pdf}
}

@unpublished{farebrotherGeneralizationRegularizationDQN2020,
  title = {Generalization and {{Regularization}} in {{DQN}}},
  author = {Farebrother, Jesse and Machado, Marlos C. and Bowling, Michael},
  date = {2020-01-17},
  eprint = {1810.00123},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.00123},
  urldate = {2022-04-26},
  abstract = {Deep reinforcement learning algorithms have shown an impressive ability to learn complex control policies in high-dimensional tasks. However, despite the ever-increasing performance on popular benchmarks, policies learned by deep reinforcement learning algorithms can struggle to generalize when evaluated in remarkably similar environments. In this paper we propose a protocol to evaluate generalization in reinforcement learning through different modes of Atari 2600 games. With that protocol we assess the generalization capabilities of DQN, one of the most traditional deep reinforcement learning algorithms, and we provide evidence suggesting that DQN overspecializes to the training environment. We then comprehensively evaluate the impact of dropout and 2 regularization, as well as the impact of reusing learned representations to improve the generalization capabilities of DQN. Despite regularization being largely underutilized in deep reinforcement learning, we show that it can, in fact, help DQN learn more general features. These features can be reused and fine-tuned on similar tasks, considerably improving DQN’s sample efficiency.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\7CYP88TG\\Farebrother et al. - 2020 - Generalization and Regularization in DQN.pdf}
}

@unpublished{finnModelAgnosticMetaLearningFast2017,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  date = {2017-07-18},
  eprint = {1703.03400},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1703.03400},
  urldate = {2022-01-13},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\YKR3TPT9\\Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf;C\:\\Users\\Florian Krone\\Zotero\\storage\\XN2AY2ZZ\\1703.html}
}

@unpublished{fuD4RLDatasetsDeep2021,
  title = {{{D4RL}}: {{Datasets}} for {{Deep Data-Driven Reinforcement Learning}}},
  shorttitle = {{{D4RL}}},
  author = {Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  date = {2021-02-05},
  eprint = {2004.07219},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2004.07219},
  urldate = {2022-01-07},
  abstract = {The offline reinforcement learning (RL) setting (also known as full batch RL), where a policy is learned from a static dataset, is compelling as progress enables RL methods to take advantage of large, previously-collected datasets, much like how the rise of large datasets has fueled results in supervised learning. However, existing online RL benchmarks are not tailored towards the offline setting and existing offline RL benchmarks are restricted to data generated by partially-trained agents, making progress in offline RL difficult to measure. In this work, we introduce benchmarks specifically designed for the offline setting, guided by key properties of datasets relevant to real-world applications of offline RL. With a focus on dataset collection, examples of such properties include: datasets generated via hand-designed controllers and human demonstrators, multitask datasets where an agent performs different tasks in the same environment, and datasets collected with mixtures of policies. By moving beyond simple benchmark tasks and data collected by partially-trained RL agents, we reveal important and unappreciated deficiencies of existing algorithms. To facilitate research, we have released our benchmark tasks and datasets with a comprehensive evaluation of existing algorithms, an evaluation protocol, and open-source examples. This serves as a common starting point for the community to identify shortcomings in existing offline RL methods and a collaborative route for progress in this emerging area.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\SPW5AXR9\\Fu et al. - 2021 - D4RL Datasets for Deep Data-Driven Reinforcement .pdf;C\:\\Users\\Florian Krone\\Zotero\\storage\\AMP7I4JD\\2004.html}
}

@misc{galDropoutBayesianApproximation2016,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  date = {2016-10-04},
  number = {arXiv:1506.02142},
  eprint = {1506.02142},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1506.02142},
  urldate = {2022-09-22},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs –extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\67MEXQFU\\Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf}
}

@unpublished{gamrianTransferLearningRelated2019,
  title = {Transfer {{Learning}} for {{Related Reinforcement Learning Tasks}} via {{Image-to-Image Translation}}},
  author = {Gamrian, Shani and Goldberg, Yoav},
  date = {2019-07-04},
  eprint = {1806.07377},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1806.07377},
  urldate = {2022-04-26},
  abstract = {Despite the remarkable success of Deep RL in learning control policies from raw pixels, the resulting models do not generalize. We demonstrate that a trained agent fails completely when facing small visual changes, and that fine-tuning—the common transfer learning paradigm—fails to adapt to these changes, to the extent that it is faster to re-train the model from scratch. We show that by separating the visual transfer task from the control policy we achieve substantially better sample efficiency and transfer behavior, allowing an agent trained on the source task to transfer well to the target tasks. The visual mapping from the target to the source domain is performed using unaligned GANs, resulting in a control policy that can be further improved using imitation learning from imperfect demonstrations. We demonstrate the approach on synthetic visual variants of the Breakout game, as well as on transfer between subsequent levels of Road Fighter, a Nintendo car-driving game. A visualization of our approach can be seen in https://youtu.be/4mnkzYyXMn4 and https://youtu.be/KCGTrQi6Ogo.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\8JKKQNLL\\Gamrian und Goldberg - 2019 - Transfer Learning for Related Reinforcement Learni.pdf}
}

@misc{gamrianTransferLearningRelated2019a,
  title = {Transfer {{Learning}} for {{Related Reinforcement Learning Tasks}} via {{Image-to-Image Translation}}},
  author = {Gamrian, Shani and Goldberg, Yoav},
  date = {2019-07-04},
  number = {arXiv:1806.07377},
  eprint = {1806.07377},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1806.07377},
  urldate = {2022-08-11},
  abstract = {Despite the remarkable success of Deep RL in learning control policies from raw pixels, the resulting models do not generalize. We demonstrate that a trained agent fails completely when facing small visual changes, and that fine-tuning—the common transfer learning paradigm—fails to adapt to these changes, to the extent that it is faster to re-train the model from scratch. We show that by separating the visual transfer task from the control policy we achieve substantially better sample efficiency and transfer behavior, allowing an agent trained on the source task to transfer well to the target tasks. The visual mapping from the target to the source domain is performed using unaligned GANs, resulting in a control policy that can be further improved using imitation learning from imperfect demonstrations. We demonstrate the approach on synthetic visual variants of the Breakout game, as well as on transfer between subsequent levels of Road Fighter, a Nintendo car-driving game. A visualization of our approach can be seen in https://youtu.be/4mnkzYyXMn4 and https://youtu.be/KCGTrQi6Ogo.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\UJV5MV4F\\Gamrian and Goldberg - 2019 - Transfer Learning for Related Reinforcement Learni.pdf}
}

@article{gouKnowledgeDistillationSurvey2021a,
  title = {Knowledge {{Distillation}}: {{A Survey}}},
  shorttitle = {Knowledge {{Distillation}}},
  author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},
  date = {2021-06-01},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {129},
  number = {6},
  pages = {1789--1819},
  issn = {1573-1405},
  doi = {10.1007/s11263-021-01453-z},
  url = {https://doi.org/10.1007/s11263-021-01453-z},
  urldate = {2022-10-31},
  abstract = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher–student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.},
  langid = {english},
  keywords = {Deep neural networks,Knowledge distillation,Knowledge transfer,Model compression,Teacher–student architecture},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\G6RNVZ26\\Gou et al. - 2021 - Knowledge Distillation A Survey.pdf}
}

@book{guReviewSafeReinforcement2022,
  title = {A {{Review}} of {{Safe Reinforcement Learning}}: {{Methods}}, {{Theory}} and {{Applications}}},
  shorttitle = {A {{Review}} of {{Safe Reinforcement Learning}}},
  author = {Gu, Shangding and Yang, Long and Du, Yali and Chen, Guang and Walter, Florian and Wang, Jun and Yang, Yaodong and Knoll, Alois},
  date = {2022-05-20},
  doi = {10.48550/arXiv.2205.10330},
  abstract = {Reinforcement learning has achieved tremendous success in many complex decision making tasks. When it comes to deploying RL in the real world, safety concerns are usually raised, leading to a growing demand for safe reinforcement learning algorithms, such as in autonomous driving and robotics scenarios. While safety control has a long history, the study of safe RL algorithms is still in the early stages. To establish a good foundation for future research in this thread, in this paper, we provide a review for safe RL from the perspectives of methods, theory and applications. Firstly, we review the progress of safe RL from five dimensions and come up with five problems that are crucial for safe RL being deployed in real-world applications, coined as "2H3W". Secondly, we analyze the theory and algorithm progress from the perspectives of answering the "2H3W" problems. Then, the sample complexity of safe RL methods is reviewed and discussed, followed by an introduction of the applications and benchmarks of safe RL algorithms. Finally, we open the discussion of the challenging problems in safe RL, hoping to inspire more future research on this thread. To advance the study of safe RL algorithms, we release a benchmark suite, an open-sourced repository containing the implementations of major safe RL algorithms, along with tutorials at the link: https://github.com/chauncygu/Safe-Reinforcement-Learning-Baselines.git.},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\CVXJMP2C\\Gu et al. - 2022 - A Review of Safe Reinforcement Learning Methods, .pdf}
}

@unpublished{haarnojaLearningWalkDeep2019,
  title = {Learning to {{Walk}} via {{Deep Reinforcement Learning}}},
  author = {Haarnoja, Tuomas and Ha, Sehoon and Zhou, Aurick and Tan, Jie and Tucker, George and Levine, Sergey},
  date = {2019-06-19},
  eprint = {1812.11103},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1812.11103},
  urldate = {2022-03-19},
  abstract = {Deep reinforcement learning (deep RL) holds the promise of automating the acquisition of complex controllers that can map sensory inputs directly to low-level actions. In the domain of robotic locomotion, deep RL could enable learning locomotion skills with minimal engineering and without an explicit model of the robot dynamics. Unfortunately, applying deep RL to real-world robotic tasks is exceptionally difficult, primarily due to poor sample complexity and sensitivity to hyperparameters. While hyperparameters can be easily tuned in simulated domains, tuning may be prohibitively expensive on physical systems, such as legged robots, that can be damaged through extensive trial-and-error learning. In this paper, we propose a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We apply this method to learning walking gaits on a real-world Minitaur robot. Our method can acquire a stable gait from scratch directly in the real world in about two hours, without relying on any model or simulation, and the resulting policy is robust to moderate variations in the environment. We further show that our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. Videos of training and the learned policy can be found on the project website3.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\NSFFS8BR\\1812.11103.pdf}
}

@article{haarnojaOffPolicyMaximumEntropy,
  title = {Off-{{Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  pages = {10},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  langid = {english},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\MRNUVDF4\\Haarnoja et al. - Off-Policy Maximum Entropy Deep Reinforcement Lear.pdf}
}

@misc{haarnojaSoftActorCriticAlgorithms2019,
  title = {Soft {{Actor-Critic Algorithms}} and {{Applications}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  date = {2019-01-29},
  number = {arXiv:1812.05905},
  eprint = {1812.05905},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1812.05905},
  url = {http://arxiv.org/abs/1812.05905},
  urldate = {2022-10-14},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\A3XCRX76\\Haarnoja et al. - 2019 - Soft Actor-Critic Algorithms and Applications.pdf;C\:\\Users\\Florian Krone\\Zotero\\storage\\NZBQNZTK\\1812.html}
}

@article{ibarzHowTrainYour2021,
  title = {How to {{Train Your Robot}} with {{Deep Reinforcement Learning}}; {{Lessons We}}'ve {{Learned}}},
  author = {Ibarz, Julian and Tan, Jie and Finn, Chelsea and Kalakrishnan, Mrinal and Pastor, Peter and Levine, Sergey},
  date = {2021-04},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {40},
  number = {4-5},
  eprint = {2102.02915},
  eprinttype = {arxiv},
  pages = {698--721},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364920987859},
  url = {http://arxiv.org/abs/2102.02915},
  urldate = {2022-04-19},
  abstract = {Deep reinforcement learning (RL) has emerged as a promising approach for autonomously acquiring complex behaviors from low level sensor observations. Although a large portion of deep RL research has focused on applications in video games and simulated control, which does not connect with the constraints of learning in real environments, deep RL has also demonstrated promise in enabling physical robots to learn complex skills in the real world. At the same time, real world robotics provides an appealing domain for evaluating such algorithms, as it connects directly to how humans learn – as an embodied agent in the real world. Learning to perceive and move in the real world presents numerous challenges, some of which are easier to address than others, and some of which are often not considered in RL research that focuses only on simulated domains. In this review article, we present a number of case studies involving robotic deep RL. Building off of these case studies, we discuss commonly perceived challenges in deep RL and how they have been addressed in these works. We also provide an overview of other outstanding challenges, many of which are unique to the real-world robotics setting and are not often the focus of mainstream RL research. Our goal is to provide a resource both for roboticists and machine learning researchers who are interested in furthering the progress of deep RL in the real world.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\Q5GFHFVW\\Ibarz et al. - 2021 - How to Train Your Robot with Deep Reinforcement Le.pdf}
}

@unpublished{ibarzRewardLearningHuman2018,
  title = {Reward Learning from Human Preferences and Demonstrations in {{Atari}}},
  author = {Ibarz, Borja and Leike, Jan and Pohlen, Tobias and Irving, Geoffrey and Legg, Shane and Amodei, Dario},
  date = {2018-11-15},
  eprint = {1811.06521},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1811.06521},
  urldate = {2022-01-07},
  abstract = {To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\U3EP8X2T\\Ibarz et al. - 2018 - Reward learning from human preferences and demonst.pdf}
}

@online{ImageNet,
  title = {{{ImageNet}}},
  url = {https://www.image-net.org/index.php},
  urldate = {2022-06-07},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\3IIQZPWA\\index.html}
}

@misc{kalashnikovQTOptScalableDeep2018,
  title = {{{QT-Opt}}: {{Scalable Deep Reinforcement Learning}} for {{Vision-Based Robotic Manipulation}}},
  shorttitle = {{{QT-Opt}}},
  author = {Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and Levine, Sergey},
  date = {2018-11-27},
  number = {arXiv:1806.10293},
  eprint = {1806.10293},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1806.10293},
  url = {http://arxiv.org/abs/1806.10293},
  urldate = {2022-07-25},
  abstract = {In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96\% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\XDRP6HRY\\Kalashnikov et al. - 2018 - QT-Opt Scalable Deep Reinforcement Learning for V.pdf;C\:\\Users\\Florian Krone\\Zotero\\storage\\ECJZVYB6\\1806.html}
}

@misc{kostrikovImageAugmentationAll2021,
  title = {Image {{Augmentation Is All You Need}}: {{Regularizing Deep Reinforcement Learning}} from {{Pixels}}},
  shorttitle = {Image {{Augmentation Is All You Need}}},
  author = {Kostrikov, Ilya and Yarats, Denis and Fergus, Rob},
  date = {2021-03-07},
  number = {arXiv:2004.13649},
  eprint = {2004.13649},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2004.13649},
  urldate = {2022-08-01},
  abstract = {We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to transform input examples, as well as regularizing the value function and policy. Existing model-free approaches, such as Soft Actor-Critic (SAC) [22], are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC’s performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based [23, 38, 24] methods and recently proposed contrastive learning [50]. Our approach, which we dub DrQ: Data-regularized Q, can be combined with any model-free reinforcement learning algorithm. We further demonstrate this by applying it to DQN [43] and significantly improve its data-efficiency on the Atari 100k [31] benchmark. An implementation can be found at https://sites. google.com/view/data-regularized-q.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\8KYXLU9W\\2004.13649.pdf}
}

@article{kullbackInformationSufficiency1951,
  title = {On {{Information}} and {{Sufficiency}}},
  author = {Kullback, S. and Leibler, R. A.},
  date = {1951},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {1},
  eprint = {2236703},
  eprinttype = {jstor},
  pages = {79--86},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\J7Q5HWGQ\\Kullback and Leibler - 1951 - On Information and Sufficiency.pdf}
}

@unpublished{leeNetworkRandomizationSimple2020,
  title = {Network {{Randomization}}: {{A Simple Technique}} for {{Generalization}} in {{Deep Reinforcement Learning}}},
  shorttitle = {Network {{Randomization}}},
  author = {Lee, Kimin and Lee, Kibok and Shin, Jinwoo and Lee, Honglak},
  date = {2020-02-15},
  eprint = {1910.05396},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.05396},
  urldate = {2022-04-26},
  abstract = {Deep reinforcement learning (RL) agents often fail to generalize to unseen environments (yet semantically similar to trained agents), particularly when they are trained on high-dimensional state spaces, such as images. In this paper, we propose a simple technique to improve a generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Furthermore, we consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. We demonstrate the superiority of our method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks: it significantly outperforms various regularization and data augmentation methods for the same purpose. Code is available at github.com/pokaxpoka/netrand.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\835EMSDY\\Lee et al. - 2020 - Network Randomization A Simple Technique for Gene.pdf}
}

@unpublished{matsushimaDeploymentEfficientReinforcementLearning2020,
  title = {Deployment-{{Efficient Reinforcement Learning}} via {{Model-Based Offline Optimization}}},
  author = {Matsushima, Tatsuya and Furuta, Hiroki and Matsuo, Yutaka and Nachum, Ofir and Gu, Shixiang},
  date = {2020-06-23},
  eprint = {2006.03647},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.03647},
  urldate = {2022-01-07},
  abstract = {Most reinforcement learning (RL) algorithms assume online access to the environment, in which one may readily interleave updates to the policy with experience collection using that policy. However, in many real-world applications such as health, education, dialogue agents, and robotics, the cost or potential risk of deploying a new data-collection policy is high, to the point that it can become prohibitive to update the data-collection policy more than a few times during learning. With this view, we propose a novel concept of deployment efficiency, measuring the number of distinct data-collection policies that are used during policy learning. We observe that na\textbackslash "\{i\}vely applying existing model-free offline RL algorithms recursively does not lead to a practical deployment-efficient and sample-efficient algorithm. We propose a novel model-based algorithm, Behavior-Regularized Model-ENsemble (BREMEN) that can effectively optimize a policy offline using 10-20 times fewer data than prior works. Furthermore, the recursive application of BREMEN is able to achieve impressive deployment efficiency while maintaining the same or better sample efficiency, learning successful policies from scratch on simulated robotic environments with only 5-10 deployments, compared to typical values of hundreds to millions in standard RL baselines. Codes and pre-trained models are available at https://github.com/matsuolab/BREMEN .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\K2UFXZKI\\Matsushima et al. - 2020 - Deployment-Efficient Reinforcement Learning via Mo.pdf;C\:\\Users\\Florian Krone\\Zotero\\storage\\BEJRYN23\\2006.html}
}

@misc{mnihPlayingAtariDeep2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  date = {2013-12-19},
  number = {arXiv:1312.5602},
  eprint = {1312.5602},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1312.5602},
  urldate = {2022-09-20},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\A93XVEFI\\Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf}
}

@unpublished{nicholFirstOrderMetaLearningAlgorithms2018,
  title = {On {{First-Order Meta-Learning Algorithms}}},
  author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
  date = {2018-10-22},
  eprint = {1803.02999},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1803.02999},
  urldate = {2022-01-17},
  abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\PK6YFPK2\\Nichol et al. - 2018 - On First-Order Meta-Learning Algorithms.pdf;C\:\\Users\\Florian Krone\\Zotero\\storage\\FBY5S6RS\\1803.html}
}

@online{openaiGymToolkitDeveloping,
  title = {Gym: {{A}} Toolkit for Developing and Comparing Reinforcement Learning Algorithms},
  shorttitle = {Gym},
  author = {OpenAI},
  url = {https://gym.openai.com},
  urldate = {2022-01-23},
  abstract = {A toolkit for developing and comparing reinforcement learning algorithms},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\4LR2KST7\\gym.openai.com.html}
}

@misc{paternainConstrainedReinforcementLearning2019,
  title = {Constrained {{Reinforcement Learning Has Zero Duality Gap}}},
  author = {Paternain, Santiago and Chamon, Luiz F. O. and Calvo-Fullana, Miguel and Ribeiro, Alejandro},
  date = {2019-10-29},
  number = {arXiv:1910.13393},
  eprint = {1910.13393},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1910.13393},
  urldate = {2022-08-08},
  abstract = {Autonomous agents must often deal with conflicting requirements, such as completing tasks using the least amount of time/energy, learning multiple tasks, or dealing with multiple opponents. In the context of reinforcement learning (RL), these problems are addressed by (i) designing a reward function that simultaneously describes all requirements or (ii) combining modular value functions that encode them individually. Though effective, these methods have critical downsides. Designing good reward functions that balance different objectives is challenging, especially as the number of objectives grows. Moreover, implicit interference between goals may lead to performance plateaus as they compete for resources, particularly when training on-policy. Similarly, selecting parameters to combine value functions is at least as hard as designing an all-encompassing reward, given that the effect of their values on the overall policy is not straightforward. The later is generally addressed by formulating the conflicting requirements as a constrained RL problem and solved using Primal-Dual methods. These algorithms are in general not guaranteed to converge to the optimal solution since the problem is not convex. This work provides theoretical support to these approaches by establishing that despite its non-convexity, this problem has zero duality gap, i.e., it can be solved exactly in the dual domain, where it becomes convex. Finally, we show this result basically holds if the policy is described by a good parametrization (e.g., neural networks) and we connect this result with primal-dual algorithms present in the literature and we establish the convergence to the optimal solution.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\QWF7HHQX\\Paternain et al. - 2019 - Constrained Reinforcement Learning Has Zero Dualit.pdf}
}

@misc{paternainSafePoliciesReinforcement2022,
  title = {Safe {{Policies}} for {{Reinforcement Learning}} via {{Primal-Dual Methods}}},
  author = {Paternain, Santiago and Calvo-Fullana, Miguel and Chamon, Luiz F. O. and Ribeiro, Alejandro},
  date = {2022-01-12},
  number = {arXiv:1911.09101},
  eprint = {1911.09101},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, math},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1911.09101},
  urldate = {2022-08-08},
  abstract = {In this paper, we study the learning of safe policies in the setting of reinforcement learning problems. This is, we aim to control a Markov Decision Process (MDP) of which we do not know the transition probabilities, but we have access to sample trajectories through experience. We define safety as the agent remaining in a desired safe set with high probability during the operation time. We therefore consider a constrained MDP where the constraints are probabilistic. Since there is no straightforward way to optimize the policy with respect to the probabilistic constraint in a reinforcement learning framework, we propose an ergodic relaxation of the problem. The advantages of the proposed relaxation are threefold. (i) The safety guarantees are maintained in the case of episodic tasks and they are kept up to a given time horizon for continuing tasks. (ii) The constrained optimization problem despite its non-convexity has arbitrarily small duality gap if the parametrization of the policy is rich enough. (iii) The gradients of the Lagrangian associated to the safe-learning problem can be easily computed using standard policy gradient results and stochastic approximation tools. Leveraging these advantages, we establish that primal-dual algorithms are able to find policies that are safe and optimal. We test the proposed approach in a navigation task in a continuous domain. The numerical results show that our algorithm is capable of dynamically adapting the policy to the environment and the required safety levels.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\2JK5TMBG\\Paternain et al. - 2022 - Safe Policies for Reinforcement Learning via Prima.pdf}
}

@inproceedings{pathakCuriosityDrivenExplorationSelfSupervised2017,
  title = {Curiosity-{{Driven Exploration}} by {{Self-Supervised Prediction}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  date = {2017-07},
  pages = {488--489},
  publisher = {{IEEE}},
  location = {{Honolulu, HI, USA}},
  doi = {10.1109/CVPRW.2017.70},
  url = {http://ieeexplore.ieee.org/document/8014804/},
  urldate = {2022-03-21},
  abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent’s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  isbn = {978-1-5386-0733-6},
  langid = {english},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\ZANMG28Z\\Pathak et al. - 2017 - Curiosity-Driven Exploration by Self-Supervised Pr.pdf}
}

@misc{phamOptLayerPracticalConstrained2018,
  title = {{{OptLayer}} - {{Practical Constrained Optimization}} for {{Deep Reinforcement Learning}} in the {{Real World}}},
  author = {Pham, Tu-Hoa and De Magistris, Giovanni and Tachibana, Ryuki},
  date = {2018-02-23},
  number = {arXiv:1709.07643},
  eprint = {1709.07643},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1709.07643},
  urldate = {2022-08-11},
  abstract = {While deep reinforcement learning techniques have recently produced considerable achievements on many decision-making problems, their use in robotics has largely been limited to simulated worlds or restricted motions, since unconstrained trial-and-error interactions in the real world can have undesirable consequences for the robot or its environment. To overcome such limitations, we propose a novel reinforcement learning architecture, OptLayer, that takes as inputs possibly unsafe actions predicted by a neural network and outputs the closest actions that satisfy chosen constraints. While learning control policies often requires carefully crafted rewards and penalties while exploring the range of possible actions, OptLayer ensures that only safe actions are actually executed and unsafe predictions are penalized during training. We demonstrate the effectiveness of our approach on robot reaching tasks, both simulated and in the real world.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\XVK6IH33\\Pham et al. - 2018 - OptLayer - Practical Constrained Optimization for .pdf}
}

@article{rayBenchmarkingSafeExploration,
  title = {Benchmarking {{Safe Exploration}} in {{Deep Reinforcement Learning}}},
  author = {Ray, Alex and Achiam, Joshua and Amodei, Dario},
  pages = {25},
  abstract = {Reinforcement learning (RL) agents need to explore their environments in order to learn optimal policies by trial and error. In many environments, safety is a critical concern and certain errors are unacceptable: for example, robotics systems that interact with humans should never cause injury to the humans while exploring. While it is currently typical to train RL agents mostly or entirely in simulation, where safety concerns are minimal, we anticipate that challenges in simulating the complexities of the real world (such as human-AI interactions) will cause a shift towards training RL agents directly in the real world, where safety concerns are paramount. Consequently we take the position that safe exploration should be viewed as a critical focus area for RL research, and in this work we make three contributions to advance the study of safe exploration. First, building on a wide range of prior work on safe reinforcement learning, we propose to standardize constrained RL as the main formalism for safe exploration. Second, we present the Safety Gym benchmark suite, a new slate of high-dimensional continuous control environments for measuring research progress on constrained RL. Finally, we benchmark several constrained deep RL algorithms on Safety Gym environments to establish baselines that future work can build on.},
  langid = {english},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\CEI7U4H6\\Ray et al. - Benchmarking Safe Exploration in Deep Reinforcemen.pdf}
}

@misc{srinivasCURLContrastiveUnsupervised2020,
  title = {{{CURL}}: {{Contrastive Unsupervised Representations}} for {{Reinforcement Learning}}},
  shorttitle = {{{CURL}}},
  author = {Srinivas, Aravind and Laskin, Michael and Abbeel, Pieter},
  date = {2020-09-21},
  number = {arXiv:2004.04136},
  eprint = {2004.04136},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2004.04136},
  urldate = {2022-08-01},
  abstract = {We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs offpolicy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at https://www. github.com/MishaLaskin/curl.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\6S3243VX\\Srinivas et al. - 2020 - CURL Contrastive Unsupervised Representations for.pdf}
}

@article{stookeDecouplingRepresentationLearning,
  title = {Decoupling {{Representation Learning}} from {{Reinforcement Learning}}},
  author = {Stooke, Adam and Lee, Kimin and Abbeel, Pieter and Laskin, Michael},
  pages = {10},
  abstract = {In an effort to overcome limitations of rewarddriven feature learning in deep reinforcement learning (RL) from images, we propose decoupling representation learning from policy learning. To this end, we introduce a new unsupervised learning (UL) task, called Augmented Temporal Contrast (ATC), which trains a convolutional encoder to associate pairs of observations separated by a short time difference, under image augmentations and using a contrastive loss. In online RL experiments, we show that training the encoder exclusively using ATC matches or outperforms end-to-end RL in most environments. Additionally, we benchmark several leading UL algorithms by pre-training encoders on expert demonstrations and using them, with weights frozen, in RL agents; we find that agents using ATC-trained encoders outperform all others. We also train multitask encoders on data from multiple environments and show generalization to different downstream RL tasks. Finally, we ablate components of ATC, and introduce a new data augmentation to enable replay of (compressed) latent images from pre-trained encoders when RL requires augmentation. Our experiments span visually diverse RL benchmarks in DeepMind Control, DeepMind Lab, and Atari, and our complete code is available at https://github.com/astooke/ rlpyt/tree/master/rlpyt/ul.},
  langid = {english},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\9IWBHDAC\\Stooke et al. - Decoupling Representation Learning from Reinforcem.pdf}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S.},
  editor = {Barto, Andrew},
  date = {2018},
  series = {Adaptive Computation and Machine Learning},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  location = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-03924-6},
  pagetotal = {xxii, 526 Seiten : Illustrationen, Diagramme ; 24 cm},
  keywords = {Künstliche Intelligenz / Maschinelles Lernen / Operante Konditionierung}
}

@misc{tesslerRewardConstrainedPolicy2018,
  title = {Reward {{Constrained Policy Optimization}}},
  author = {Tessler, Chen and Mankowitz, Daniel J. and Mannor, Shie},
  date = {2018-12-26},
  number = {arXiv:1805.11074},
  eprint = {1805.11074},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1805.11074},
  urldate = {2022-07-31},
  abstract = {Solving tasks in Reinforcement Learning is no easy feat. As the goal of the agent is to maximize the accumulated reward, it often learns to exploit loopholes and misspecifications in the reward signal resulting in unwanted behavior. While constraints may solve this issue, there is no closed form solution for general constraints. In this work, we present a novel multi-timescale approach for constrained policy optimization, called ‘Reward Constrained Policy Optimization’ (RCPO), which uses an alternative penalty signal to guide the policy towards a constraint satisfying one. We prove the convergence of our approach and provide empirical evidence of its ability to train constraint satisfying policies.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\9MKNQLUY\\Tessler et al. - 2018 - Reward Constrained Policy Optimization.pdf}
}

@misc{wachiSafeReinforcementLearning2020,
  title = {Safe {{Reinforcement Learning}} in {{Constrained Markov Decision Processes}}},
  author = {Wachi, Akifumi and Sui, Yanan},
  date = {2020-08-14},
  number = {arXiv:2008.06626},
  eprint = {2008.06626},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2008.06626},
  urldate = {2022-08-11},
  abstract = {Safe reinforcement learning has been a promising approach for optimizing the policy of an agent that operates in safety-critical applications. In this paper, we propose an algorithm, SNO-MDP, that explores and optimizes Markov decision processes under unknown safety constraints. Specifically, we take a stepwise approach for optimizing safety and cumulative reward. In our method, the agent first learns safety constraints by expanding the safe region, and then optimizes the cumulative reward in the certified safe region. We provide theoretical guarantees on both the satisfaction of the safety constraint and the near-optimality of the cumulative reward under proper regularity assumptions. In our experiments, we demonstrate the effectiveness of SNO-MDP through two experiments: one uses a synthetic data in a new, openlyavailable environment named GP-SAFETY-GYM, and the other simulates Mars surface exploration by using real observation data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\JD96LTNL\\Wachi and Sui - 2020 - Safe Reinforcement Learning in Constrained Markov .pdf}
}

@misc{wagenerSafeReinforcementLearning2021,
  title = {Safe {{Reinforcement Learning Using Advantage-Based Intervention}}},
  author = {Wagener, Nolan and Boots, Byron and Cheng, Ching-An},
  date = {2021-07-19},
  number = {arXiv:2106.09110},
  eprint = {2106.09110},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2106.09110},
  urldate = {2022-08-11},
  abstract = {Many sequential decision problems involve finding a policy that maximizes total reward while obeying safety constraints. Although much recent research has focused on the development of safe reinforcement learning (RL) algorithms that produce a safe policy after training, ensuring safety during training as well remains an open problem. A fundamental challenge is performing exploration while still satisfying constraints in an unknown Markov decision process (MDP). In this work, we address this problem for the chanceconstrained setting. We propose a new algorithm, SAILR, that uses an intervention mechanism based on advantage functions to keep the agent safe throughout training and optimizes the agent’s policy using off-the-shelf RL algorithms designed for unconstrained MDPs. Our method comes with strong guarantees on safety during both training and deployment (i.e., after training and without the intervention mechanism) and policy performance compared to the optimal safetyconstrained policy. In our experiments, we show that SAILR violates constraints far less during training than standard safe RL and constrained MDP approaches and converges to a wellperforming policy that can be deployed safely without intervention. Our code is available at https://github.com/nolanwagener/safe\_rl.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\77Q2HM5U\\Wagener et al. - 2021 - Safe Reinforcement Learning Using Advantage-Based .pdf}
}

@online{WelcomeSpinningDeep,
  title = {Welcome to {{Spinning Up}} in {{Deep RL}}! — {{Spinning Up}} Documentation},
  url = {https://spinningup.openai.com/en/latest/},
  urldate = {2022-09-26},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\3UH6NRJT\\latest.html}
}

@misc{yangProjectionBasedConstrainedPolicy2020,
  title = {Projection-{{Based Constrained Policy Optimization}}},
  author = {Yang, Tsung-Yen and Rosca, Justinian and Narasimhan, Karthik and Ramadge, Peter J.},
  date = {2020-10-07},
  number = {arXiv:2010.03152},
  eprint = {2010.03152},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2010.03152},
  urldate = {2022-07-18},
  abstract = {We consider the problem of learning control policies that optimize a reward function while satisfying constraints due to considerations of safety, fairness, or other costs. We propose a new algorithm, Projection-Based Constrained Policy Optimization (PCPO). This is an iterative method for optimizing policies in a two-step process: the first step performs a local reward improvement update, while the second step reconciles any constraint violation by projecting the policy back onto the constraint set. We theoretically analyze PCPO and provide a lower bound on reward improvement, and an upper bound on constraint violation, for each policy update. We further characterize the convergence of PCPO based on two different metrics: \$\textbackslash normltwo\$ norm and Kullback-Leibler divergence. Our empirical results over several control tasks demonstrate that PCPO achieves superior performance, averaging more than 3.5 times less constraint violation and around 15\textbackslash\% higher reward compared to state-of-the-art methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\JTV7X698\\Yang et al. - 2020 - Projection-Based Constrained Policy Optimization.pdf}
}

@misc{yangProjectionBasedConstrainedPolicy2020a,
  title = {Projection-{{Based Constrained Policy Optimization}}},
  author = {Yang, Tsung-Yen and Rosca, Justinian and Narasimhan, Karthik and Ramadge, Peter J.},
  date = {2020-10-07},
  number = {arXiv:2010.03152},
  eprint = {2010.03152},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2010.03152},
  urldate = {2022-08-08},
  abstract = {We consider the problem of learning control policies that optimize a reward function while satisfying constraints due to considerations of safety, fairness, or other costs. We propose a new algorithm, Projection-Based Constrained Policy Optimization (PCPO). This is an iterative method for optimizing policies in a two-step process: the first step performs a local reward improvement update, while the second step reconciles any constraint violation by projecting the policy back onto the constraint set. We theoretically analyze PCPO and provide a lower bound on reward improvement, and an upper bound on constraint violation, for each policy update. We further characterize the convergence of PCPO based on two different metrics: \$\textbackslash normltwo\$ norm and Kullback-Leibler divergence. Our empirical results over several control tasks demonstrate that PCPO achieves superior performance, averaging more than 3.5 times less constraint violation and around 15\textbackslash\% higher reward compared to state-of-the-art methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\QPUC3L6E\\Yang et al. - 2020 - Projection-Based Constrained Policy Optimization.pdf}
}

@unpublished{yaratsImprovingSampleEfficiency2020,
  title = {Improving {{Sample Efficiency}} in {{Model-Free Reinforcement Learning}} from {{Images}}},
  author = {Yarats, Denis and Zhang, Amy and Kostrikov, Ilya and Amos, Brandon and Pineau, Joelle and Fergus, Rob},
  date = {2020-07-09},
  eprint = {1910.01741},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.01741},
  urldate = {2022-02-27},
  abstract = {Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. A promising approach is to learn a latent representation together with the control policy. However, fitting a high-capacity encoder using a scarce reward signal is sample inefficient and leads to poor performance. Prior work has shown that auxiliary losses, such as image reconstruction, can aid efficient representation learning. However, incorporating reconstruction loss into an off-policy learning algorithm often leads to training instability. We explore the underlying reasons and identify variational autoencoders, used by previous investigations, as the cause of the divergence. Following these findings, we propose effective techniques to improve training stability. This results in a simple approach capable of matching state-of-the-art model-free and model-based algorithms on MuJoCo control tasks. Furthermore, our approach demonstrates robustness to observational noise, surpassing existing approaches in this setting. Code, results, and videos are anonymously available at https://sites.google.com/view/sac-ae/home.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\IFMCDAXN\\Yarats et al. - 2020 - Improving Sample Efficiency in Model-Free Reinforc.pdf}
}

@misc{yuConvergentPolicyOptimization2019,
  title = {Convergent {{Policy Optimization}} for {{Safe Reinforcement Learning}}},
  author = {Yu, Ming and Yang, Zhuoran and Kolar, Mladen and Wang, Zhaoran},
  date = {2019-10-26},
  number = {arXiv:1910.12156},
  eprint = {1910.12156},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1910.12156},
  urldate = {2022-08-08},
  abstract = {We study the safe reinforcement learning problem with nonlinear function approximation, where policy optimization is formulated as a constrained optimization problem with both the objective and the constraint being nonconvex functions. For such a problem, we construct a sequence of surrogate convex constrained optimization problems by replacing the nonconvex functions locally with convex quadratic functions obtained from policy gradient estimators. We prove that the solutions to these surrogate problems converge to a stationary point of the original nonconvex problem. Furthermore, to extend our theoretical results, we apply our algorithm to examples of optimal control and multi-agent reinforcement learning with safety constraints.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\EJ2VUINU\\Yu et al. - 2019 - Convergent Policy Optimization for Safe Reinforcem.pdf}
}

@unpublished{yuMetaWorldBenchmarkEvaluation2021,
  title = {Meta-{{World}}: {{A Benchmark}} and {{Evaluation}} for {{Multi-Task}} and {{Meta Reinforcement Learning}}},
  shorttitle = {Meta-{{World}}},
  author = {Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Narayan, Avnish and Shively, Hayden and Bellathur, Adithya and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  date = {2021-06-14},
  eprint = {1910.10897},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.10897},
  urldate = {2022-01-13},
  abstract = {Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\T4QSYN7S\\Yu et al. - 2021 - Meta-World A Benchmark and Evaluation for Multi-T.pdf}
}

@misc{zhanFrameworkEfficientRobotic2020,
  title = {A {{Framework}} for {{Efficient Robotic Manipulation}}},
  author = {Zhan, Albert and Zhao, Philip and Pinto, Lerrel and Abbeel, Pieter and Laskin, Michael},
  date = {2020-12-14},
  number = {arXiv:2012.07975},
  eprint = {2012.07975},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2012.07975},
  urldate = {2022-07-04},
  abstract = {Data-efficient learning of manipulation policies from visual observations is an outstanding challenge for realrobot learning. While deep reinforcement learning (RL) algorithms have shown success learning policies from visual observations, they still require an impractical number of real-world data samples to learn effective policies. However, recent advances in unsupervised representation learning and data augmentation significantly improved the sample efficiency of training RL policies on common simulated benchmarks. Building on these advances, we present a Framework for Efficient Robotic Manipulation (FERM) that utilizes data augmentation and unsupervised learning to achieve extremely sample-efficient training of robotic manipulation policies with sparse rewards. We show that, given only 10 demonstrations, a single robotic arm can learn sparse-reward manipulation policies from pixels, such as reaching, picking, moving, pulling a large object, flipping a switch, and opening a drawer in just 15-50 minutes of real-world training time. We include videos, code, and additional information on the project website – https://sites.google.com/view/ efficient-robotic-manipulation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\YMQMQNGQ\\Zhan et al. - 2020 - A Framework for Efficient Robotic Manipulation.pdf}
}

@unpublished{zhangStudyOverfittingDeep2018,
  title = {A {{Study}} on {{Overfitting}} in {{Deep Reinforcement Learning}}},
  author = {Zhang, Chiyuan and Vinyals, Oriol and Munos, Remi and Bengio, Samy},
  date = {2018-04-20},
  eprint = {1804.06893},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1804.06893},
  urldate = {2022-04-26},
  abstract = {Recent years have witnessed significant progresses in deep Reinforcement Learning (RL). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging RL problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep RL techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard RL agents and find that they could overfit in various ways. Moreover, overfitting could happen “robustly”: commonly used techniques in RL that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in RL. We conclude with a general discussion on overfitting in RL and a study of the generalization behaviors from the perspective of inductive bias.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\DVXDGK2L\\Zhang et al. - 2018 - A Study on Overfitting in Deep Reinforcement Learn.pdf}
}

@unpublished{zhaoOfflineMetaReinforcementLearning2021,
  title = {Offline {{Meta-Reinforcement Learning}} for {{Industrial Insertion}}},
  author = {Zhao, Tony Z. and Luo, Jianlan and Sushkov, Oleg and Pevceviciute, Rugile and Heess, Nicolas and Scholz, Jon and Schaal, Stefan and Levine, Sergey},
  date = {2021-10-12},
  eprint = {2110.04276},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2110.04276},
  urldate = {2022-01-07},
  abstract = {Reinforcement learning (RL) can in principle make it possible for robots to automatically adapt to new tasks, but in practice current RL methods require a very large number of trials to accomplish this. In this paper, we tackle rapid adaptation to new tasks through the framework of metalearning, which utilizes past tasks to learn to adapt, with a specific focus on industrial insertion tasks. We address two specific challenges by applying meta-learning in this setting. First, conventional meta-RL algorithms require lengthy online meta-training phases. We show that this can be replaced with appropriately chosen offline data, resulting in an offline metaRL method that only requires demonstrations and trials from each of the prior tasks, without the need to run costly metaRL procedures online. Second, meta-RL methods can fail to generalize to new tasks that are too different from those seen at meta-training time, which poses a particular challenge in industrial applications, where high success rates are critical. We address this by combining contextual meta-learning with direct online finetuning: if the new task is similar to those seen in the prior data, then the contextual meta-learner adapts immediately, and if it is too different, it gradually adapts through finetuning. We show that our approach is able to quickly adapt to a variety of different insertion tasks, learning how to perform them with a success rate of 100\% using only a fraction of the samples needed for learning the tasks from scratch. Experiment videos and details are available at https://sites.google. com/view/offline-metarl-insertion.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\AP5CRD3V\\Zhao et al. - 2021 - Offline Meta-Reinforcement Learning for Industrial.pdf}
}

@unpublished{zhuLearningSparseRewarded2020,
  title = {Learning {{Sparse Rewarded Tasks}} from {{Sub-Optimal Demonstrations}}},
  author = {Zhu, Zhuangdi and Lin, Kaixiang and Dai, Bo and Zhou, Jiayu},
  date = {2020-04-01},
  eprint = {2004.00530},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2004.00530},
  urldate = {2022-05-09},
  abstract = {Model-free deep reinforcement learning (RL) has demonstrated its superiority on many complex sequential decision-making problems. However, heavy dependence on dense rewards and high sample-complexity impedes the wide adoption of these methods in real-world scenarios. On the other hand, imitation learning (IL) learns effectively in sparse-rewarded tasks by leveraging the existing expert demonstrations. In practice, collecting a sufficient amount of expert demonstrations can be prohibitively expensive, and the quality of demonstrations typically limits the performance of the learning policy. In this work, we propose Self-Adaptive Imitation Learning (SAIL) that can achieve (near) optimal performance given only a limited number of sub-optimal demonstrations for highly challenging sparse reward tasks. SAIL bridges the advantages of IL and RL to reduce the sample complexity substantially, by effectively exploiting sup-optimal demonstrations and efficiently exploring the environment to surpass the demonstrated performance. Extensive empirical results show that not only does SAIL significantly improve the sample-efficiency but also leads to much better final performance across different continuous control tasks, comparing to the state-of-the-art.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\4CHSE2W8\\Zhu et al. - 2020 - Learning Sparse Rewarded Tasks from Sub-Optimal De.pdf}
}

@unpublished{zhuTransferLearningDeep2021,
  title = {Transfer {{Learning}} in {{Deep Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Transfer {{Learning}} in {{Deep Reinforcement Learning}}},
  author = {Zhu, Zhuangdi and Lin, Kaixiang and Zhou, Jiayu},
  date = {2021-03-04},
  eprint = {2009.07888},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2009.07888},
  urldate = {2022-05-09},
  abstract = {Reinforcement Learning (RL) is a key technique to address sequential decision-making problems and is crucial to realize advanced artificial intelligence. Recent years have witnessed remarkable progress in RL by virtue of the fast development of deep neural networks. Along with the promising prospects of RL in numerous domains, such as robotics and game-playing, transfer learning has arisen as an important technique to tackle various challenges faced by RL, by transferring knowledge from external expertise to accelerate the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible RL backbones, and practical applications. We also draw connections between transfer learning and other relevant topics from the RL perspective and explore their potential challenges as well as open questions that await future research progress.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Florian Krone\\Zotero\\storage\\BAUM2K5H\\Zhu et al. - 2021 - Transfer Learning in Deep Reinforcement Learning .pdf}
}
